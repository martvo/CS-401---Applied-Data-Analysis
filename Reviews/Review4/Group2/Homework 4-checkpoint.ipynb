{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Obsevational Studies and Applied ML\n",
    "\n",
    "### Deadline\n",
    "November 21st,11:59PM\n",
    "\n",
    "### Important notes\n",
    "\n",
    "Make sure you push on GitHub your notebook with all the cells already evaluated. Don't forget to add a textual description of your thought process, the assumptions you made, and the solution you implemented. Back up any hypotheses and claims with data, since this is an important aspect of the course. Please write all your comments in English, and use meaningful variable names in your code. Your repo should have a single notebook (plus the data files necessary) in the master branch. If there are multiple notebooks present, we will not grade anything.\n",
    "\n",
    "Use this legendary link to create your repository: [link](https://classroom.github.com/g/YXtsr0QK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Boosting the economy by incentivizing self-employment\n",
    "\n",
    "Assume the biggest priority of the local government in 2018 is to increase per-capita income. To do so, the officials plan to adopt a strategy for incentivizing self-employment through a series of campaigns, educational programs, and dedicated funds.\n",
    "\n",
    "Since it is unethical and impossible in this setting to run a controlled experiment involving citizens (e.g., fire employees and force them to self-employ), the officials have asked you, the data scientist, to establish the effect of self-employment on the economy, relying on observational data.\n",
    "\n",
    "**A)** You will be working with the full US 2015 census dataset (acs2015_county_data.csv, available at https://www.kaggle.com/muonneutrino/us-census-demographic-data#acs2015_county_data.csv). Using suitable methods, determine and quantify the dependency between the percentage of self-employed citizens and per capita income across all 3,212 US counties. Do citizens in counties that have a higher percentage of self-employed people earn more per capita?\n",
    "\n",
    "**B)** The pilot program will involve all counties within a limited set of three US states. Set A includes Wisconsin, Tennessee, and  Minnesota. Quantify the dependency of per-capita income on self-employment rates across all the counties in set A.\n",
    "\n",
    "**C)** In which state within set A is the observed effect of self-employment on per-capita income the strongest?\n",
    "\n",
    "**D)** Set B includes New Jersey, Kansas, and Rhode Island. Repeat the analysis from steps B and C above, but now for set B. In which of the two sets A and B (if any) would you recommend incentivizing self-employment? Explain your reasoning.\n",
    "\n",
    "Hint: It is useful to add a notion of confidence to your results and explore the data visually. You are allowed to use the SciPy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame\n",
    "df = pd.read_csv(data_folder +'acs2015_county_data.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056413673053189874\n",
      "0.08727386609551786\n"
     ]
    }
   ],
   "source": [
    "# A\n",
    "\n",
    "# Percentace of self-employed ['SelfEmployed']\n",
    "# Per capita income in a county ['IncomePerCap']\n",
    "\n",
    "# compute two correlat methods\n",
    "spearman_corr = df['SelfEmployed'].corr(df['IncomePerCap'], method='spearman')\n",
    "pearson_corr, _ = pearsonr(df['SelfEmployed'], df['IncomePerCap'])\n",
    "print(corr)\n",
    "print(pearson_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation Part A:\n",
    "By looking at the correlation methods used, spearman and pearson, we can see that both show an **very low correlation** between the **Self Employment percentage** in a county and its **Income Per Capita**\n",
    "Therefore, counties with higher percentage of of self-employed people do not necessarily earn more per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set A Spearman: 0.056413673053189874\n",
      "Set A Pearson -0.20229350736521498\n",
      "Wisconsin Spearman: -0.46351291044049403 0.004768134887745234\n",
      "Wisconsin Pearson -0.32905300016378525\n",
      "Tennessee Spearman: -0.316991392780988\n",
      "Tennessee Pearson -0.23836048684913141\n",
      "Minnesota Spearman: -0.21107460598245847\n",
      "Minnesota Pearson -0.2538551921654062\n"
     ]
    }
   ],
   "source": [
    "# B \n",
    "\n",
    "a = ['Wisconsin', 'Tennessee', 'Minnesota']\n",
    "\n",
    "# Compute three states all together\n",
    "set_a = df[df['State'].isin(a)]\n",
    "spearman_corr = set_a['SelfEmployed'].corr(set_a['IncomePerCap'], method='spearman')\n",
    "pearson_corr, _ = pearsonr(set_a['SelfEmployed'], set_a['IncomePerCap'])\n",
    "print('Set A Spearman:',corr)\n",
    "print('Set A Pearson',pearson_corr)\n",
    "\n",
    "# Compute each state individually\n",
    "wisconsin_df = df[df['State'] == 'Wisconsin']\n",
    "tennessee_df = df[df['State'] == 'Tennessee']\n",
    "minnesote_df = df[df['State'] == 'Minnesota']\n",
    "\n",
    "# Wisconsin\n",
    "spearman_corr = wisconsin_df['SelfEmployed'].corr(wisconsin_df['IncomePerCap'], method='spearman')\n",
    "pearson_corr, _ = pearsonr(wisconsin_df['SelfEmployed'], wisconsin_df['IncomePerCap'])\n",
    "print('Wisconsin Spearman:', spearman_corr)\n",
    "print('Wisconsin Pearson', pearson_corr)\n",
    "\n",
    "# Tennessee\n",
    "spearman_corr = tennessee_df['SelfEmployed'].corr(tennessee_df['IncomePerCap'], method='spearman')\n",
    "pearson_corr, _ = pearsonr(tennessee_df['SelfEmployed'], tennessee_df['IncomePerCap'])\n",
    "print('Tennessee Spearman:', spearman_corr)\n",
    "print('Tennessee Pearson', pearson_corr)\n",
    "\n",
    "# Minnesota\n",
    "spearman_corr = minnesote_df['SelfEmployed'].corr(minnesote_df['IncomePerCap'], method='spearman')\n",
    "pearson_corr, _ = pearsonr(minnesote_df['SelfEmployed'], minnesote_df['IncomePerCap'])\n",
    "print('Minnesota Spearman:', spearman_corr)\n",
    "print('Minnesota Pearson', pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Part B\n",
    "\n",
    "### Set A\n",
    "\n",
    "**Spearman: 0.056413673053189874** <br>\n",
    "**Pearson: -0.20229350736521498**\n",
    "\n",
    "After computing the correlations using Spearman and Pearson, we can see that the former method gives us a value (0.056) closer to zero than to -1 and 1. This means that there's almost no dependency between the percentage of self-employed people and the income per capita of these three states. On the other hand, when using the Pearson correlation, we can see a slight negative correlation from the value obtained (-0.20); this means that in a slight way the more self-employed people there are, the less the income per capita will be.\n",
    "\n",
    "Next, we see the correlations between the self-employed percentage and income per capita in each state.\n",
    "\n",
    "###  Wisconsin\n",
    "**Spearman: -0.46351291044049403** <br>\n",
    "**Pearson -0.32905300016378525**\n",
    "\n",
    "### Tennessee\n",
    "**Spearman: -0.316991392780988** <br>\n",
    "**Pearson -0.23836048684913141**\n",
    "\n",
    "### Minnesota\n",
    "**Spearman: -0.21107460598245847** <br>\n",
    "**Pearson -0.2538551921654062**\n",
    "\n",
    "In each state we see a negative correlation between the self-employed percentage and income per capita. As previously exaplained, this means that the more self-employed people there are, the less the income per capita will be. In Wisconsin we can see a higher negative correlation between the two features, indicating an even stronger negative correlation. The other two states are slighly lower but still inversely proportional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 2: All you need is love… And a dog!\n",
    "\n",
    "Here we are going to build a classifier to predict whether an animal from an animal shelter will be adopted or not (aac_intakes_outcomes.csv, available at: https://www.kaggle.com/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes/version/1#aac_intakes_outcomes.csv). You will be working with the following features:\n",
    "\n",
    "1. *animal_type:* Type of animal. May be one of 'cat', 'dog', 'bird', etc.\n",
    "2. *intake_year:* Year of intake\n",
    "3. *intake_condition:* The intake condition of the animal. Can be one of 'normal', 'injured', 'sick', etc.\n",
    "4. *intake_number:* The intake number denoting the number of occurrences the animal has been brought into the shelter. Values higher than 1 indicate the animal has been taken into the shelter on more than one occasion.\n",
    "5. *intake_type:* The type of intake, for example, 'stray', 'owner surrender', etc.\n",
    "6. *sex_upon_intake:* The gender of the animal and if it has been spayed or neutered at the time of intake\n",
    "7. *age_upon\\_intake_(years):* The age of the animal upon intake represented in years\n",
    "8. *time_in_shelter_days:* Numeric value denoting the number of days the animal remained at the shelter from intake to outcome.\n",
    "9. *sex_upon_outcome:* The gender of the animal and if it has been spayed or neutered at time of outcome\n",
    "10. *age_upon\\_outcome_(years):* The age of the animal upon outcome represented in years\n",
    "11. *outcome_type:* The outcome type. Can be one of ‘adopted’, ‘transferred’, etc.\n",
    "\n",
    "**A)** Load the dataset and convert categorical features to a suitable numerical representation (use dummy-variable encoding). Split the data into a training set (80%) and a test set (20%). Pair each feature vector with the corresponding label, i.e., whether the outcome_type is adoption or not. Standardize the values of each feature in the data to have mean 0 and variance 1. The use of external libraries is not permitted in part A, except for numpy and pandas.\n",
    "\n",
    "**B)** Train a logistic regression classifier on your training set. Logistic regression returns probabilities as predictions, so in order to arrive at a binary prediction, you need to put a threshold on the predicted probabilities. For the decision threshold of 0.5, present the performance of your classifier on the test set by displaying the confusion matrix. Based on the confusion matrix, manually calculate accuracy, precision, recall, and F1-score with respect to the positive and the negative class. Vary the value of the threshold in the range from 0 to 1 and visualize the value of accuracy, precision, recall, and F1-score (with respect to both classes) as a function of the threshold. The shelter has a limited capacity and has no other option but to put to sleep animals with a low probability of adoption. What metric (precision, recall, accuracy, or F1-score) and with respect to what class is the most relevant when choosing the threshold in this scenario, and why? Explain your reasoning.\n",
    "\n",
    "**C)** Reduce the number of features by selecting the subset of the k best features. Use greedy backward selection to iteratively remove features. Evaluate performance and visualize the result using 5-fold cross-validation on the training set as a function of k, where k = 1, 5, 10, 15, 20, 25, 30. Choose the optimal k and justify your choice. Interpret the top-k features and their impact on the probability of adoption.\n",
    "\n",
    "**D)** Train a random forest. Use 5-fold cross-validation on the training set to fine-tune the parameters of the classifier using a grid search on the number of estimators \"n_estimators\" and the max depth of the trees \"max_depth\". For the chosen parameters, estimate the performance of your classifier on the test set by presenting the confusion matrix, accuracy, precision, recall, and F1-score with respect to both classes and compare the performance with the performance of the logistic regression. Interpret the results.\n",
    "\n",
    "You are allowed to use the scikit-learn library to implement your classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
